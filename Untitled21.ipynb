{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Подход к решению задачи\n",
        "\n",
        "Для решения задачи определения тональности сообщений в соцсетях на основе предоставленного датасета можно использовать подход, основанный на обработке естественного языка и глубоком обучении. Сначала необходимо произвести предобработку текста, такую как токенизация, приведение к одному регистру, удаление стоп-слов и лемматизация. Затем можно использовать эмбеддинги слов для преобразования слов в числовые векторы и передачи их на вход нейронной сети. Сами же данные можно разбить на обучающую и тестовую выборки. Нейронная сеть может состоять из нескольких слоев, например: Embedding слой, Conv1D слои, MaxPooling1D слои, слой Flatten, Dense слои и выходной слой с функцией активации softmax для классификации на 3 класса.\n",
        "\n",
        "2. Обучение модели\n",
        "\n",
        "Для обучение модели на предоставленном датасете можно использовать библиотеки Keras или PyTorch. Для удобства мы использовали Keras.\n",
        "\n",
        "3. Измерение точности модели на тестовом датасете\n",
        "\n",
        "Точность модели на тестовом датасете можно измерить с помощью метода evaluate, который вычисляет потери и метрики модели на тестовом наборе данных. В примере выше, мы использовали этот метод и вывели точность модели на тестовом датасете.\n",
        "\n",
        "4. Интерпретация модели\n",
        "\n",
        "Хотя нейронные сети могут использоваться для классификации текста, они могут быть трудны для интерпретации. Однако можно предположить, что наличие определенных слов или фраз может указывать на конкретную тональность. Например, использование слов \"хорошо\", \"понравилось\" может указывать на то, что текст имеет позитивную тональность, а использование слов \"плохо\", \"неприятно\" может указывать на негативную тональность. Однако, понимание того, какие слова или фразы именно повлияли на принятие решения модели, может быть сложной задачей для интерпретации.\n",
        "\n",
        "Таким образом, для интерпретации модели нейронной сети можно использовать методы визуализации такие как активации слоев, глубокое внедрение, сетевые веса и т.д. Эти методы могут помочь понять, какие функции и слои модели используются для принятия решения.\n",
        "\n",
        "Кроме того, есть методы вычисления важности признаков (например, LIME, SHAP), которые объясняют, почему конкретный текст был отнесен к негативному или позитивному классу. Эти методы можно использовать для интерпретации модели и понимания ее работы.\n"
      ],
      "metadata": {
        "id": "j40Im3snr7dx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "EUTKtKPlsC2W"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Pandas - библиотека для работы с таблицами данных (или DataFrame) в Python. Она предоставляет инструменты для обработки, фильтрации и анализа данных. Pandas позволяет импортировать и экспортировать данные из различных форматов, таких как CSV, Excel, SQL и многих других. Она также предоставляет возможность очистки данных, заполнения пропущенных значений и объединения нескольких таблиц данных.\n",
        "\n",
        "2. NumPy - библиотека для научных вычислений в Python. Она предоставляет быстрые и эффективные методы для работы с массивами данных, включая поддержку алгебраических операций, матричных операций, случайных чисел и многих других. NumPy также предоставляет множество функций для анализа данных и математических вычислений.\n",
        "\n",
        "3. Keras - высокоуровневый API для создания и обучения нейронных сетей в Python. Keras позволяет быстро и легко строить нейронные сети с различными архитектурами и слоями. Она предоставляет простой интерфейс для создания и обучения моделей нейронных сетей, а также инструменты для оценки производительности моделей.\n",
        "\n",
        "4. Conv1D - слой свертки для использования в нейронных сетях. Он используется для извлечения признаков из последовательных данных, таких как временные ряды, аудио- и видеоданные. Conv1D работает аналогично сверточным слоям в изображениях, но применяется к одномерным данным.\n",
        "\n",
        "5. MaxPooling1D - слой пулинга, также известный как субдискретизация. Он используется для уменьшения размерности данных, что помогает уменьшить количество параметров модели и ускорить обучение. MaxPooling1D используется для нахождения максимальных значений в окне, которое перемещается по дискретному сигналу. Результатом становится уменьшенный по размеру сигнал.\n",
        "\n",
        "6. Flatten - слой, который используется для преобразования многомерных данных в одномерный вектор. Он преобразует выходной тензор из предыдущего слоя в одномерный вектор, который может быть использован в качестве входа для полносвязной нейронной сети.\n",
        "\n",
        "7. Embedding - слой, который используется для преобразования целых чисел (слов) в плотные векторные представления. Embedding слой включает в себя матрицу весов, которые постепенно обучаются в процессе обучения модели. Это позволяет искать отношения между словами и создавать более эффективное представление данных для нейронных сетей.\n",
        "\n",
        "8. Tokenizer - класс из библиотеки Keras, который используется для преобразования текстового корпуса в последовательности чисел (так называемый индекс). Он создает словарь всех слов в корпусе, для каждого слова присваивается уникальный индекс, и затем каждое предложение представляется в виде последовательности индексов.\n",
        "\n",
        "9. pad_sequences - функция из библиотеки Keras, которая используется для заполнения или усечения последовательности индексов до фиксированной длины. Эта функция полезна, когда длина предложений в корпусе различна, а нейронные сети требуют входных данных одинаковой длины. Функция pad_sequences дополняет короткие последовательности нулями и усекает длинные последовательности до заданной максимальной длины.\n",
        "\n",
        "10. train_test_split - функция из библиотеки scikit-learn, которая используется для разбиения данных на обучающую и тестовую выборки. Она принимает в качестве аргументов данные и целевую переменную (метки классов), и возвращает четыре набора данных: обучающие данные, тестовые данные, обучающие метки и тестовые метки. Разбиение данных на обучающую и тестовую выборки позволяет оценить производительность модели на новых данных, которые не использовались в процессе обучения.\n",
        "\n",
        "Все эти библиотеки и функции являются незаменимыми для работы с нейронными сетями и машинным обучением в Python. Они позволяют создавать и обрабатывать большие объемы данных, а также преобразовывать данные в форму, которую можно использовать для обучения моделей."
      ],
      "metadata": {
        "id": "ywL3dDcDtmzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/datasets/Tatyana/ru_sentiment_dataset/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJTqX7rlyBpP",
        "outputId": "b140e7cd-095a-4025-8fac-97269d779f70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ru_sentiment_dataset'...\n",
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Total 18 (delta 0), reused 0 (delta 0), pack-reused 18\u001b[K\n",
            "Unpacking objects: 100% (18/18), 2.79 KiB | 714.00 KiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Загружаем датасет и преобразуем его в Pandas DataFrame\n",
        "\n",
        "data = pd.read_csv(\"ru_sentiment_dataset/train.csv\")#, delimiter=\"\\t\")\n",
        "data = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "ntaoF61ctroF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Производим предобработку текста, используя токенизацию и лемматизацию\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data[\"text\"])\n",
        "sequences = tokenizer.texts_to_sequences(data[\"text\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "TaxUOzXJuKAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import sequence"
      ],
      "metadata": {
        "id": "Uq2SQle0zi0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = tokenizer.word_index\n",
        "vocab_size  = len(word_index) + 1\n",
        "max_len = 50\n",
        "X = sequence.pad_sequences(sequences, maxlen=max_len)\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "y = data[\"sentiment\"].values\n",
        "y = to_categorical(y, num_classes=3)"
      ],
      "metadata": {
        "id": "ntITc2V0zjxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Разбиваем данные на обучающую и тестовую выборки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "_OEtiYkkulOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. `data = pd.read_csv(\"ru_sentiment_dataset.csv\", delimiter=\"\\t\")` - мы загружаем датасет, который представляет собой файл в формате CSV, берём его из текущей директории и читаем его с помощью функции `read_csv` библиотеки Pandas. Мы используем разделитель \"\\t\" (табуляция), потому что наш файл разделён попавление по столбцам с помощью табуляции.\n",
        "\n",
        "2. `data = pd.DataFrame(data)` - мы создаём объект DataFrame из прочитанных данных. Метод `pd.DataFrame()` используется для преобразования данных, прочитанных из CSV, в объект DataFrame.\n",
        "\n",
        "3. `tokenizer = Tokenizer()` - мы создаём объект Tokenizer из библиотеки Keras, который используется для токенизации текста.\n",
        "\n",
        "4. `tokenizer.fit_on_texts(data[\"text\"])` - мы обучаем Tokenizer на текстах в датасете. Tokenizer использует функцию `fit_on_texts`, чтобы создать индекс всех уникальных слов в текстовых колонках.\n",
        "\n",
        "5. `sequences = tokenizer.texts_to_sequences(data[\"text\"])` - мы преобразуем тексты датасета в последовательности чисел (индексы слов), используя ранее созданный объект Tokenizer. Тексты, записанные в столбце \"text\", разбиваются на отдельные слова, каждому слову присваивается индекс из словаря, созданного методом `fit_on_texts`, и затем создаётся последовательность чисел (индексов слов) для каждого текста.\n",
        "\n",
        "6. `word_index = tokenizer.word_index` - мы получаем словарь всех уникальных слов в текстах, созданных с помощью метода `fit_on_texts`. Каждому слову присваивается его индекс в словаре.\n",
        "\n",
        "7. `vocab_size = len(word_index) + 1` - мы устанавливаем размер словаря (также называемый размером словаря), добавляя 1 к количеству слов, чтобы обеспечить наличие индекса для пустых слов. Также установка этого значения позволяет создать соответствующий размер для матрицы embedding-слоя.\n",
        "\n",
        "8. `max_len = 50` - мы устанавливаем максимальную длину последовательности (максимальное количество слов в тексте), которую будем использовать для обучения модели.\n",
        "\n",
        "9. `X = pad_sequences(sequences, maxlen=max_len)` - мы используем функцию pad_sequences из библиотеки Keras для заполнения или усечения каждой последовательности чисел (индексов слов) до фиксированной длины `max_len`. Таким образом, мы приводим длину всех последовательностей к одной и той же длине.\n",
        "\n",
        "10. `y = data[\"label\"].values` - мы извлекаем столбец меток классов и преобразуем его в массив numpy.\n",
        "\n",
        "11. `X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)` - мы разбиваем данные на обучающую и тестовую выборки, используя функцию train_test_split из библиотеки scikit-learn. Здесь мы используем параметр `test_size=0.2` для того, чтобы отвести 20% данных для тестовой выборки. Установка `random_state=42` гарантирует, что разбиение данных будет фиксированным и повторяемым при каждом запуске кода."
      ],
      "metadata": {
        "id": "N9HpGZhauPbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем модель нейронной сети\n",
        "embedding_size = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_size, input_length=max_len))\n",
        "model.add(Conv1D(64, 5, activation=\"relu\"))\n",
        "model.add(MaxPooling1D(pool_size=4))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation=\"relu\"))\n",
        "model.add(Dense(3, activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "8zDXNtXIvelo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade keras_preprocessing"
      ],
      "metadata": {
        "id": "dlhMBPr1224X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. `embedding_size = 100` - мы устанавливаем размерность пространства, в которое будут векторизоваться слова. Здесь мы устанавливаем значение 100.\n",
        "\n",
        "2. `model = Sequential()` - мы создаём пустую последовательную модель нейронной сети, которая будет состоять из нескольких слоёв.\n",
        "\n",
        "3. `model.add(Embedding(vocab_size, embedding_size, input_length=max_len))` - мы добавляем слой Embedding, который используется для векторизации слов. В этом слое каждому индексу слова из словаря (vocab_size) присваивается вектор заданной размерности (embedding_size). Входной параметр input_length определяет фиксированную длину последовательности, паддинг и обрезка не производится.\n",
        "\n",
        "4. `model.add(Conv1D(64, 5, activation=\"relu\"))` - мы добавляем слой свёртки (Convolution) с 64 нейронами, окном размера 5 и функцией активации ReLU. Этот слой используется для обнаружения локальных признаков во входных данных.\n",
        "\n",
        "5. `model.add(MaxPooling1D(pool_size=4))` - мы добавляем слой субдискретизации (MaxPooling), который используется для уменьшения размерности выходных данных путём выбора максимального значения в окне размера 4. Это помогает сократить вычислительные затраты и улучшить обобщающую способность модели.\n",
        "\n",
        "6. `model.add(Flatten())` - мы добавляем слой Flatten, который используется для превращения выходных данных из предыдущих слоёв в одномерный вектор.\n",
        "\n",
        "7. `model.add(Dense(128, activation=\"relu\"))` - мы добавляем полносвязный слой (Dense) с 128 нейронами и функцией активации ReLU. Этот слой используется для того, чтобы объединить и обработать низкоуровневые признаки, полученные на предыдущем Conv1D слое.\n",
        "\n",
        "8. `model.add(Dense(3, activation=\"softmax\"))` - мы добавляем финальный полносвязный слой (Dense) с 3 нейронами и функцией активации softmax, который используется для вычисления вероятности принадлежности каждого входного текста к одному из трёх классов (негативный, нейтральный, положительный).\n",
        "\n",
        "9. `model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])` - мы компилируем модель, выбирая функцию потерь categorical_crossentropy для решения задачи классификации с несколькими классами (трёхклассовая классификация), оптимизатор Adam для обучения модели и метрику accuracy для оценки производительности.\n",
        "\n",
        "10. `print(model.summary())` - мы выводим информацию о модели, используя метод summary. Выводится архитектура модели и количество параметров.\n",
        "\n",
        "Этот код создаёт и компилирует модель нейронной сети, которая будет использоваться для классификации текстов по эмоциональной окраске (негативный, нейтральный и положительный). Она состоит из нескольких слоёв: Embedding, Conv1D, MaxPooling1D, Flatten, Dense."
      ],
      "metadata": {
        "id": "3Ky12uxbwb8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для того чтобы модель могла классифицировать новые данные, необходимо обучить её на обучающих данных `X_train` и `y_train`. После этого можно использовать метод `predict()` для получения прогнозов классов на новых данных и вывести 0 или 1 для каждого примера"
      ],
      "metadata": {
        "id": "Qh4N9yIQwhmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs=1)\n",
        "core = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"test accuracy = \", score[1]*100, \"%\")"
      ],
      "metadata": {
        "id": "2DKMldLWCgDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Классифицируем новые данные и выводим 0 или 1 для каждого примера\n",
        "new_data = [\"Этот фильм просто потрясающий!\", \"К сожалению, сервис оказался непригодным для использования.\",\n",
        "            \"Сегодня я чувствую себя просто замечательно!\"]\n",
        "new_sequences = tokenizer.texts_to_sequences(new_data)\n",
        "new_X = sequence.pad_sequences(new_sequences, maxlen=max_len)\n",
        "predict_classes = np.argmax(model.predict(new_X), axis=-1)\n",
        "for i, predict_class in enumerate(predict_classes):\n",
        "    print(f\"Пример {i+1}: {predict_class}\")"
      ],
      "metadata": {
        "id": "ffDL4Z3Q0sDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zVERzzRpC4Dh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}